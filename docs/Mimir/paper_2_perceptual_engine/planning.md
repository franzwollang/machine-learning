# Paper 3: Companion Paper - Perceptual & Temporal Processing

## Title

"The Delphic Engine: Genesis of Predictive Structure from Raw Multimodal Streams"

## Guiding Principles & Caveats

- **Focus on Standalone Value:** This paper should be framed as a general-purpose technique for unsupervised temporal analysis. While developed for Mimir, its value must be demonstrated independently. It should briefly mention its potential for integration into larger cognitive architectures as motivation, but the core contribution should not depend on that integration.
- **Explain Shared Mechanisms:** This paper must also independently introduce and explain the core learning methodology (the "local objectives, unrolling, global error" approach) and the nature of the shared ANFIS-based modules as they apply to perceptual and temporal processing.

## Shared Technical Foundations (To be detailed in each paper)

This section outlines the core methodology used to build the Delphic Engine. To ensure this paper is self-contained as a general-purpose technique for sequence analysis, we detail these specific techniques here.

### 1. ANFIS-based Modular Toolkit

The Delphic Engine's generative power comes from a specific composition of adaptive neuro-fuzzy inference system (ANFIS) based modules. This neuro-fuzzy approach provides a unique blend of powerful representation learning and interpretable predictive structure. The key modules are:

- **Elastic ANFIS Autoencoder:** Used as the core of the "Sequential Student" model. At each time step `t`, it learns a compressed, salient state representation `h_t` from the raw, high-dimensional input `x_t` of a multimodal sequence. Its elasticity is key for handling diverse and potentially novel modalities without manual reconfiguration.
- **DNF-DNN (Disjunctive Normal Form DNN):** This module takes the hidden state `h_t` and learns explicit, interpretable predictive rules to generate a prediction `x̂_{t+1}` for the next step in the sequence. The DNF-DNN's ability to isolate multiple high-importance logical rules allows the engine to model complex sequence dynamics where multiple distinct futures are possible.

### 2. Unified Learning Framework: Local & Global Objectives

The Delphic Engine is trained using a dual-level strategy, which allows it to be both a powerful standalone tool and an integrable component.

- **Local Objectives (Primary Operation):** The primary local objective for the Delphic Engine is next-step prediction. The complete module is trained end-to-end to minimize the reconstruction error between its generated next state `x̂_{t+1}` and the true next state `x_{t+1}`.
- **Proteus Integration (Hierarchical Pattern Discovery):** This is the core of the "genesis" process. A separate `Proteus-Temporal` instance is not trained via gradient descent but instead consumes the sequence of hidden states `h_t` generated by the ANFIS modules. By performing unsupervised topological clustering on these state sequences, Proteus discovers recurring, long-term patterns and builds a hierarchical model of the temporal data. This discovers the "predictive structure" mentioned in the title.
- **Global Unrolling & Fine-Tuning (Integration Hooks):** While the Delphic Engine is powerful on its own, it is designed to be integrable. Its entire predictive process can be "unrolled" into a differentiable graph. When connected to a downstream task (e.g., a reinforcement learning agent), a global error signal from that task can be backpropagated to the Delphic Engine. This fine-tunes its predictive and state-representation models to focus on features and sequences that are most salient to the downstream task, creating a more efficient and goal-directed perceptual system.

## Core Contribution

This paper details the perceptual front-end of the Mimir cognitive architecture. It presents a two-part system for processing sequential data: 1) The **Temporal Dynamics Engine (TDE)**, which ensures short-term predictive coherence, and 2) **`Proteus-Temporal`**, a specialized instance of the Proteus engine that learns long-term, abstract patterns from sensory input streams. The core innovation is the seamless integration of a local, frame-to-frame predictor with a global, hierarchical pattern miner.

## Target Audience

Conferences/journals focused on representation learning, sequence modeling, or unsupervised learning (e.g., ICLR, AISTATS, UAI).

## High-Level Structure

### 1. Abstract

- Motivate the need for models that can handle both short-term and long-term temporal dependencies in an unsupervised manner.
- Introduce the TDE and `Proteus-Temporal` as a combined solution.
- State the key result: the ability to discover meaningful, hierarchical patterns in complex, unlabeled sequential data.

### 2. Introduction

- **Reference the Foundational Paper:** Cite the main Mimir architecture paper and position this work as the detailed explanation of its perceptual subsystem.
- **The Challenge of Temporal Data:** Discuss the limitations of models that are good at next-step prediction but fail to capture higher-level structure, and vice-versa.
- **Proposed Solution:** A two-part architecture where a local predictor (TDE) handles immediate continuity, while a separate Proteus instance (`Proteus-Temporal`) handles global pattern abstraction.

### 3. The Perceptual Processing Architecture (Deep Dive)

- **Source Material:** `paper_final.md`
- **Overall Data Flow:** Describe how sensory input is processed first by the TDE and then fed into `Proteus-Temporal`.
- **The Temporal Dynamics Engine (TDE):**
  - **Role:** Short-term prediction and coherence.
  - **Mechanism:** Detail the "Sequential Student" model. Explain how it takes the current state and predicts the next, ensuring a smooth, continuous stream of consciousness.
- **`Proteus-Temporal`:**
  - **Role:** Long-term, hierarchical pattern discovery.
  - **Mechanism:** Explain that this is a standard application of the Proteus engine, but applied specifically to the stream of hidden states from the TDE. Describe how it autonomously discovers recurring sequences and abstracts them into a hierarchy of temporal patterns (what the old papers called "Sketches" and "Schemas").

### 4. Experiments & Results

- **Goal:** Demonstrate the system's ability to find meaningful structure in unlabeled time-series data.
- **Dataset:** Use a complex, unlabeled dataset. Examples: video feeds of traffic, recordings of birdsong, or logs of user activity on a website.
- **Experiment 1: TDE Performance.** Show that the TDE produces coherent, low-error short-term predictions.
- **Experiment 2: `Proteus-Temporal` Discovery.** Analyze the hierarchical structures discovered by `Proteus-Temporal`. Show that the learned patterns correspond to meaningful, real-world events in the data (e.g., a "traffic jam" pattern, a specific bird's song motif, a common user workflow).
- **Experiment 3: Comparison.** Compare the discovered patterns to those from other unsupervised time-series methods (e.g., an autoencoder on sequence chunks) to show the superior quality and interpretability of the Proteus-based approach.

### 5. Discussion

- Analyze the discovered hierarchies.
- Discuss the synergistic relationship between the TDE and `Proteus-Temporal`.
- Explain the benefits of this unsupervised approach (no need for manual labeling of events).

### 6. Conclusion

- Summarize the proposed perceptual architecture.
- Reiterate its value as a powerful, general-purpose engine for unsupervised discovery in temporal data, both within the Mimir system and as a standalone technique.
